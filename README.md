# Metal Parts Lifespan â€” Regression & Classification (COMP1801)

Two-task ML project for a manufacturing dataset:
1) **Regression** â€” predict lifetime (hours) of metal parts.
2) **Classification** â€” label parts into **Low / Medium / High** lifespan groups from K-Means, then build a supervised model.

## ðŸ”§ Tech & Methods
- **Pipeline**: `ColumnTransformer(StandardScaler + OneHotEncoder)` â†’ model
- **Split**: 70/15/15 (train/val/test)
- **Imbalance**: `SMOTE` (classification)
- **Models**: RandomForest, MLPRegressor / MLPClassifier
- **Tuning**: `RandomizedSearchCV` / `GridSearchCV`
- **Explainability**: Feature importances, SHAP (for NN)

## ðŸ“Š Results (highlights)
**Regression (test set):**
- Random Forest (tuned): **RÂ² â‰ˆ 0.910**, **RMSE â‰ˆ 96.58**, **MAE â‰ˆ 74.57**  
- MLP Regressor (tuned): **RÂ² â‰ˆ 0.820**, **RMSE â‰ˆ 138.82**, **MAE â‰ˆ 108.11**

**Classification (test set):**
- Random Forest: **Accuracy â‰ˆ 0.847**  
- MLP Classifier: **Accuracy â‰ˆ 0.807**

> Key drivers: **coolingRate**, **castType**, **seedLocation**; non-linear interactions present.

## ðŸ“‚ Structure
```
comp1801-metal-parts-lifespan-ml/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â”œâ”€ notebooks/
â”‚  â””â”€ COMP1801.ipynb
â”œâ”€ src/                 # (optional future: modular code)
â”œâ”€ data/
â”‚  â””â”€ README.md
â”œâ”€ assets/              # export charts for README
â””â”€ reports/
   â”œâ”€ COMP1801_Colab_Notebook.pdf
   â”œâ”€ COMP1801_Coursework_1.pdf
   â””â”€ COMP1801_Coursework_2.pdf
```

## â–¶ï¸ Run locally
```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
jupyter lab
```
Open `notebooks/COMP1801.ipynb`, set the dataset path, and run.

## â–¶ï¸ Run in Google Colab
- Upload the notebook.
- Install deps at the top cell:
  ```python
  !pip install -q scikit-learn imbalanced-learn shap seaborn
  ```
- Mount/import your dataset CSV, then run all cells.


## ðŸ” Notes
- K-Means used to derive **Low/Medium/High** labels; only the **High** cluster mostly meets the >1500h requirement (some edge cases just below threshold).  
- Random Forest recommended for classification (balanced accuracy, interpretability).  
- For regression, Random Forest achieved the strongest test metrics; NN chosen for interpretability of categorical signals in some reports.

## âœ… Next improvements
- Calibrated classification thresholds to enforce the 1500h rule.
- Add **permutation importance** + SHAP for both tasks.
- Track experiments with a seed and a fixed train/val/test split artifact.
